---
---

@string{aps = {American Physical Society,}}

@inproceedings{DRS2024,
  author    = {Zhengtao Ma and Lissy Hatfield and Chipp Jansen and Boyuan Tuo and Elif Ozden Yenigun and Sharon Baurley and Stephen Jia Wang and Kun Pyo Lee},
  title     = {Workshopping the Textile Hand: Reimagining Subjective Assessment of Textile Materials with Digital Technologies},
  booktitle = {DRS2024},
  location  = {Boston, USA},
  editor    = {Gray, C. and Ciliotta Chehade, E. and Hekkert, P. and Forlano, L. and Ciuccarelli, P. and Lloyd, P.},
  year      = {2024},
  url       = {https://doi.org/10.21606/drs.2024.536},
  doi       = {10.21606/drs.2024.536},
}


@article{Emo-MG,
author = {Le Fang, Sark Pangrui Xing, Zhengtao Ma, Zhijie Zhang, Yonghao Long, Kun-Pyo Lee and Stephen Jia Wang},
title = {Emo-MG Framework: LSTM-based Multi-modal Emotion Detection through Electroencephalography Signals and Micro Gestures},
journal = {International Journal of Human–Computer Interaction},
volume = {0},
number = {0},
pages = {1--17},
year = {2023},
publisher = {Taylor \& Francis},
doi = {10.1080/10447318.2023.2228983},
URL = {
        https://doi.org/10.1080/10447318.2023.2228983
},
eprint = {  
        https://doi.org/10.1080/10447318.2023.2228983
}
,
abstract = { Human-computer interaction has seen growing interest in emotion detection. To gain deeper insights into the physiological indicators of emotions, researchers have delved into utilizing electroencephalography (EEG) and micro-gestures (MGs). This study assesses the efficacy of EEG and MG features in emotion detection by recruiting 15 participants to gather EEG and MG data in response to diverse figure-based emotional stimuli. To incorporate these features, this article introduces Emo-MG, a multimodal interface that integrates EEG and MG features and employs a long short-term memory (LSTM) model to predict emotional states within the valence-arousal-dominance (VAD) space. This study presents an in-depth analysis of feature importance and correlation results based on EEG and MG features for feature selection in emotion detection tasks. Through accuracy and F1-score metrics, Emo-MG achieves outstanding performance in emotion detection by comparing it to baseline and deep learning models, validating the efficacy of integrating EEG and MG features }
}


@inproceedings{cscw2023poster1,
author = {Zhang, Mingyuan and Cheng, Zhaolin and Shiu, Sheung Ting Ramona and Liang, Jiacheng and Fang, Cong and Ma, Zhengtao and Fang, Le and Wang, Stephen Jia},
title = {Towards Human-Centred AI-Co-Creation: A Three-Level Framework for Effective Collaboration between Human and AI},
year = {2023},
isbn = {9798400701290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584931.3607008},
doi = {10.1145/3584931.3607008},
abstract = {AI-based creativity-support systems are gaining attention from designers and researchers. However, a research gap exists on how to tailor those systems by maximizing flexibility based on human needs and preferences. This study proposes a schematic human-AI co-creation framework to maximize system flexibility and enhance creative outcome generation. The framework proposes the involvement of AI in three levels of creation and allows humans to adjust between the levels anytime during the creative process based on their preferences. We tentatively define how AI should collaborate at the three levels. To implement the framework, a co-creation system (GSM) was built to support humans in creating sculpture maquettes with AI. It includes three key components: a prompt-based generated model (DALL·E), advanced computer vision, and robotic arms. A user interface is provided to ensure transparency. Preliminary user studies have demonstrated that the system enhances flexibility and allows users to generate more creative maquettes.},
booktitle = {Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {312–316},
numpages = {5},
keywords = {human-centered co-creation, artificial intelligence},
location = {Minneapolis, MN, USA},
series = {CSCW '23 Companion}
}

@inproceedings{HRI2024lbreport,
author = {Jansen, Chipp and Ma, Zhengtao and Hatfield, Lissy and Tuo, Boyuan and Ozden Yenigun, Elif and Baurley, Sharon and Wang, Stephen Jia and Lee, Kun Pyo},
title = {Textile Robotic Interaction for Designer-Robot Collaboration},
year = {2024},
isbn = {9798400703232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610978.3640722},
doi = {10.1145/3610978.3640722},
abstract = {This late-breaking report describes lab-based robot experiments involving two robot arms scanning and interaction with a set of 12 novel sustainable materials programmed with handfeel gestures inspired by how designers evaluate textile materials. The aim of gathering this data is to spur research in robot perception of soft materials and to contribute towards human-robot collaborative design systems. The complete dataset including scanned images, video of interactions accompanied by the robot motion paths is available with code at https://github.com/rca-msrc/textile-robotic-interaction-HRI2024.},
booktitle = {Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {563–567},
numpages = {5},
keywords = {material identification, robot gestures, robotic perception, textiles assessment},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@inproceedings{cscw2023poster2,
author = {Liu, Linyi and Yang, Siqi and Wang, Yixue and Ma, Zhengtao and Fang, Le and Long, Yonghao and Wang, Stephen Jia},
title = {Robotic Systems in Heritage Protection: An Anti-Fatigue Human-Robot Collaboration Exploration for Heritage Painting and Calligraphy Restoration},
year = {2023},
isbn = {9798400701290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584931.3606994},
doi = {10.1145/3584931.3606994},
abstract = {The heritage restoration of paintings and calligraphy is a labour-intensive process, characterised by repetitive, low-intensity, and high-precision. Two core steps, “lift up the previous” and “fix and maintain”, can lead to muscle fatigue in the upper limbs. In this paper, we develop a human-in-the-loop based human-robot collaboration system that provides real-time force feedback to restorers based on their muscle load and motion intention, with the support strength changing to relieve fatigue. Different robot-aided positions on human arm are explored and a two-point elbow-supporting mechanism is evaluated as the optimal solution. Moreover, a novel muscle load calculation model is developed to integrate human factors into the HRC system with electromyography. The results of a usability experiment show the effectiveness of the proposed method in addressing the current anti-fatigue challenge in heritage paintings and calligraphy restoration.},
booktitle = {Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {201–205},
numpages = {5},
keywords = {Anti-Fatigue, Electromyography, Heritage Restoration, Human-Robot Collaboration},
location = {Minneapolis, MN, USA},
series = {CSCW '23 Companion}
}